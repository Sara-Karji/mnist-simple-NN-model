# -*- coding: utf-8 -*-
"""mnist-simple-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1okYjo2kIr6dKOG82n34IFrkiQeJxIMS3
"""

import numpy as np
from tensorflow import keras
import tensorflow as tf
from tensorflow.keras.constraints import max_norm
from tensorflow.keras.utils import to_categorical
from keras.datasets import mnist
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from tensorflow.keras.layers import BatchNormalization
from keras.src.engine.training import optimizer
from keras.src.layers.attention.multi_head_attention import activation
import pandas as pd

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train shape", X_train.shape)
print("y_train shape", y_train.shape)
print("X_test shape", X_test.shape)
print("y_test shape", y_test.shape)

y_train

X_train.shape [1:]

# normalize each value for each pixel for the entire vector for each input # Normalize the inputs from 0-255 to between 0 and 1 by dividing by 255
X_test = X_test/255
X_train = X_train/255

plt.imshow(X_train[0])

plt.imshow(X_train[0], cmap='Greys')



y_train[0]

model_lr = Sequential([
    layers. Input ( X_train.shape[1:]),
    layers. Flatten(),
    layers.Dense(10, activation='softmax')
])
model_lr. compile(optimizer ='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_lr.summary()

y_onehot_train = tf.one_hot(y_train, 10)
numpy.ModuleDeprecationWarning

model_lr.fit(X_train, y_onehot_train)

model_lr = Sequential([
    layers. Input ( X_train.shape[1:]),
    layers. Flatten(),
    layers.Dense(10, activation='softmax')
])
model_lr. compile(optimizer ='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_lr.summary()

#model_lr.fit(X_train, y_train)

#model_lr.fit(X_train, y_train)

#model_lr.fit(X_train, y_train, epochs=10, batch_size=128, validation_split= 0.2)

history_lr = model_lr.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))

#history_lr.history
for key,val in history_lr.history.items():
  print(key)

pd.DataFrame(history_lr.history).plot()

plt.plot(history_lr.history['loss'], label='train')
plt.plot(history_lr.history['val_loss'], label='val')
plt.ylabel('loss')
plt.legend()
plt.show ()

plt.plot(history_lr.history['accuracy'],label='train')
plt.plot(history_lr.history['val_accuracy'], label='val')
plt.ylabel('accuracy')
plt.legend()
plt.show ()

probs = model_lr.predict(X_test[:8])
preds = np.argmax(probs, axis=1)

plt.rcParams['figure.figsize'] = (10,10)

for i in range(8):
    plt.subplot(1,8,i+1)
    plt.imshow(X_test[i], cmap="Greys")
    plt.title(preds[i])

plt.tight_layout()

probs = model_lr.predict(X_test[:2])
preds = np.argmax(probs, axis=1)
for i in range(2):
    print(probs[i], " => ", preds[i])
    plt.imshow(X_test[i], cmap="Greys")
    plt.show()

model_lr.predict(x_test[18].reshape(1,28,28))



"""
# **Project Report: Building and Training a Simple Neural Network for MNIST Digit Classification**
**Introduction**

In this project, I have developed a simple neural network model for the classification of handwritten digits using the MNIST dataset. The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision, consisting of 28x28 grayscale images of handwritten digits (0-9).

The main goals of this project were as follows:

Load and preprocess the MNIST dataset.

1.   Design a simple neural network model.
2.   Train the model to classify digits.
3.   Evaluate the model's performance.
4.   Visualize the results.


**Dataset Overview**

The MNIST dataset contains two sets: a training set with 60,000 images and a test set with 10,000 images. Each image is a 28x28 pixel grayscale image of a handwritten digit. The corresponding labels indicate the digit (0-9) represented in each image.

**Dataset Dimensions:**

Training Data:

*   Images: 60,000 samples
*   Labels: 60,000 labels


Test Data:

*   Images: 10,000 samples
*   Labels: 10,000 labels

**Data Preprocessing**

Before training the neural network, the following preprocessing steps were performed on the dataset:

Normalization: The pixel values in the images were normalized from the range [0, 255] to the range [0, 1] by dividing each pixel value by 255. This standardizes the input data.

**Neural Network Model**

**Logistic Regression Model**

I first created a simple logistic regression model. This model consists of a single Dense layer with 10 units, representing the 10 possible digit classes (0-9). The activation function used is softmax, which converts the model's raw output into class probabilities.



python

model_lr = Sequential([

    layers.Input(X_train.shape[1:]),
    layers.Flatten(),
    layers.Dense(10, activation='softmax')
])


**Model Compilation**

The model was compiled with the following configurations:

*   Optimizer: Adam
*   Loss Function: Categorical Crossentropy (as one-hot encoded labels were used)

*   Metrics: Accuracy

**Model Training**

The logistic regression model was trained on the training dataset. The training process involved the following settings:

*   Number of Epochs: 10
*   Batch Size: 128
*   Validation Data: Test dataset (for evaluating model performance during training)


**Results and Visualization**

The logistic regression model achieved the following results:

*   Training Accuracy: 92.76%
*   Validation Accuracy: 92.63%


**Loss and Accuracy Plots**

The loss and accuracy plots were visualized to assess the training progress and model performance. These plots were generated for both training and validation datasets. The training loss decreased, and accuracy increased over epochs, indicating successful training.

**Prediction and Visualization**

Finally, the model was used to predict digits from a subset of test images. The model's predictions were displayed alongside the corresponding images. The predictions were generally accurate, demonstrating the model's ability to classify handwritten digits.

**Conclusion**

In this project, I built and trained a simple logistic regression model for MNIST digit classification. The model achieved an accuracy of approximately 92.63% on the validation dataset, indicating its effectiveness in recognizing handwritten digits. This project serves as a basic introduction to image classification using neural networks and can be extended with more complex models and techniques for further improvements in accuracy."""